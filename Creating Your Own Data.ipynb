{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a71d1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    153\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files done\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# put your path below\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m df.head()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'path'"
     ]
    }
   ],
   "source": [
    "import biotite.structure.io.pdbx as pdbx\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from biotite.sequence import ProteinSequence, NucleotideSequence, GeneralSequence\n",
    "\n",
    "amino_acids = ['H', 'R', 'K', 'I', 'F', 'L', 'W', 'A', 'M',\n",
    "               'P', 'C', 'N', 'V', 'G', 'S', 'Q', 'Y', 'D', 'E', 'T']\n",
    "\n",
    "output_csv = \"protein_frame.csv\"\n",
    "\n",
    "headers = [\"FileName\", \"FilePath\", \"SpeciesSciName\", \"FullSequence\", \"SequenceByChain\",\n",
    "           \"NumOfChains\", \"ChainNames\", \"UniqueAminoAcids\",\n",
    "           \"Helix_Count\", \"Beta_Count\", \"Coil_Count\",\n",
    "           \"HelixByChain\", \"BetaByChain\", \"CoilByChain\", \"ProteinChain\", \"NucleaotideChain\", \"GeneralChain\", \"HelixSeq\",\"SSEARRAY\"]\n",
    "\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Darsh\\Downloads\\store-sales-time-series-forecasting\")\n",
    "\n",
    "def count_elements_by_type(ss_array, element_code):\n",
    "    count = 0\n",
    "    inside = False\n",
    "    for ss in ss_array:\n",
    "        if ss == element_code and not inside:\n",
    "            count += 1\n",
    "            inside = True\n",
    "        elif ss != element_code:\n",
    "            inside = False\n",
    "    return count\n",
    "def extract_species_name(cif_path):\n",
    "    keyword = \"_entity_src_gen.pdbx_gene_src_scientific_name\"\n",
    "    with open(cif_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith(keyword):\n",
    "            parts = line.split(keyword, 1)\n",
    "            if len(parts) > 1:\n",
    "                return parts[1].strip().strip('\"')\n",
    "    return \"N/A\"\n",
    "def filter_sse(sequence, ss_elements, exclude_types=('b', 'c')):\n",
    "    result = {}\n",
    "    for chain_id in sequence:\n",
    "        seq = str(sequence[chain_id])\n",
    "        sse_array = ss_elements[chain_id]\n",
    "        chain_result = []\n",
    "        start = 0\n",
    "        for i in range(1, len(seq) + 1):\n",
    "            if i == len(seq) or sse_array[i] != sse_array[start]:\n",
    "                sse_type = sse_array[start]\n",
    "                if sse_type not in exclude_types:\n",
    "                    group_seq = seq[start:i]\n",
    "                    chain_result.append(group_seq)\n",
    "                start = i\n",
    "        result[chain_id] = chain_result\n",
    "    return result\n",
    "with open(output_csv, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    file_count = 0\n",
    "    for cif_file_path in list(glob.glob('**/*.cif'))[:5]:\n",
    "        total_counts = {\"helix\": 0, \"beta\": 0, \"coil\": 0}\n",
    "        aa_counts = {acid: {\"helix\": 0, \"beta\": 0, \"coil\": 0} for acid in amino_acids}\n",
    "        try:\n",
    "            with open(cif_file_path, \"r\") as file:\n",
    "                cif_file = pdbx.CIFFile.read(file)\n",
    "            sequence_by_chain = pdbx.get_sequence(cif_file)\n",
    "            sequence = pdbx.get_sequence(cif_file)\n",
    "            ss_elements = pdbx.get_sse(cif_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse {cif_file_path}: {e}\")\n",
    "            continue\n",
    "        Helix_chain = {\n",
    "            chain: count_elements_by_type(ss_elements[chain], \"a\")\n",
    "            for chain in sequence_by_chain.keys()\n",
    "        }\n",
    "        Beta_chain = {\n",
    "            chain: count_elements_by_type(ss_elements[chain], \"b\")\n",
    "            for chain in sequence_by_chain.keys()\n",
    "        }\n",
    "        Coil_chain = {\n",
    "            chain: count_elements_by_type(ss_elements[chain], \"c\")\n",
    "            for chain in sequence_by_chain.keys()\n",
    "        }\n",
    "\n",
    "        for chain_arr in ss_elements.values():\n",
    "            total_counts[\"helix\"] += np.count_nonzero(chain_arr == \"a\")\n",
    "            total_counts[\"beta\"] += np.count_nonzero(chain_arr == \"b\")\n",
    "            total_counts[\"coil\"] += np.count_nonzero(chain_arr == \"c\")\n",
    "\n",
    "        for chain_id, seq in sequence.items():\n",
    "            seq_str = str(seq)\n",
    "            ss_arr = ss_elements.get(chain_id)\n",
    "            if ss_arr is None:\n",
    "                continue\n",
    "            for i, residue in enumerate(seq_str):\n",
    "                if i < len(ss_arr):\n",
    "                    ss_type = ss_arr[i]\n",
    "                    if residue in aa_counts:\n",
    "                        if ss_type == \"a\":\n",
    "                            aa_counts[residue][\"helix\"] += 1\n",
    "                        elif ss_type == \"b\":\n",
    "                            aa_counts[residue][\"beta\"] += 1\n",
    "                        elif ss_type == \"c\":\n",
    "                            aa_counts[residue][\"coil\"] += 1\n",
    "\n",
    "        file_name = os.path.splitext(os.path.basename(cif_file_path))[0]\n",
    "        full_seq = \"\".join(str(seq) for seq in sequence_by_chain.values())\n",
    "        unique_aas = sorted(set(full_seq))\n",
    "        file_aa_counts = {f\"{aa}_Count\": full_seq.count(aa) for aa in amino_acids}\n",
    "        seq_by_chain_str = \"; \".join([f\"{chain}:{seq}\" for chain, seq in sequence_by_chain.items()])\n",
    "        Chain_Names = \"; \".join(sequence_by_chain.keys())\n",
    "        helix_values = [aa_counts[acid][\"helix\"] for acid in amino_acids]\n",
    "        beta_values = [aa_counts[acid][\"beta\"] for acid in amino_acids]\n",
    "        coil_values = [aa_counts[acid][\"coil\"] for acid in amino_acids]\n",
    "        protein_types = {chain_id: isinstance(chain_seq, ProteinSequence) for chain_id, chain_seq in pdbx.get_sequence(cif_file).items()}\n",
    "        nucelotide_types = { chain_id: isinstance(chain_seq, NucleotideSequence) for chain_id, chain_seq in pdbx.get_sequence(cif_file).items()}\n",
    "        General_types = {chain_id: isinstance(chain_seq, GeneralSequence) for chain_id, chain_seq in pdbx.get_sequence(cif_file).items()}\n",
    "        Helix_seq = filter_sse(sequence, ss_elements)\n",
    "        ssarray = ''.join(\n",
    "            s\n",
    "            for arr in ss_elements.values()\n",
    "            for s in arr\n",
    "            if s in {'c', 'a', 'b'}\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"FileName\": file_name,\n",
    "            \"FilePath\": os.path.abspath(cif_file_path),\n",
    "            \"FullSequence\": full_seq,\n",
    "            \"SequenceByChain\": seq_by_chain_str,\n",
    "            \"NumOfChains\": len(sequence_by_chain),\n",
    "            \"ChainNames\": Chain_Names,\n",
    "            \"UniqueAminoAcids\": \"\".join(unique_aas),\n",
    "            \"Helix_Count\": helix_values,\n",
    "            \"Beta_Count\": beta_values,\n",
    "            \"Coil_Count\": coil_values,\n",
    "            \"HelixByChain\": Helix_chain,\n",
    "            \"BetaByChain\": Beta_chain,\n",
    "            \"CoilByChain\": Coil_chain,\n",
    "            \"ProteinChain\": protein_types,\n",
    "            \"NucleaotideChain\": nucelotide_types,\n",
    "            \"GeneralChain\": General_types,\n",
    "            \"SpeciesSciName\": extract_species_name(cif_file_path),\n",
    "            \"HelixSeq\" : Helix_seq,\n",
    "            \"SSEARRAY\" : ssarray\n",
    "            \n",
    "        }\n",
    "        writer.writerow(row)\n",
    "        file_count +=1\n",
    "        if file_count % 1000 == 0:\n",
    "            print(f\"{file_count} files done\")\n",
    "# put your csv path below\n",
    "df = pd.read_csv(r\"path\")\n",
    "df.head()\n",
    "import pandas as pd\n",
    "excel_file = 'ProteinDF.xlsx'\n",
    "df.to_excel(excel_file, index=False, engine='openpyxl')\n",
    "# you can now use this to upload your own data on the dashboard "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
